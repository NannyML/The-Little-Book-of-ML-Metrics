\chapter{Clustering}


% ---------- Mutual Info Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Mutual Info Score}
\subsection{Mutual Info Score}

The Mutual Information (MI) Score can be used to quantify the amount of information shared between two clustering assignments.
It's also particularly useful for comparing the similarity between ground-truth labels and predicted clustering labels.

% Formula
\begin{center}
    FORMULA GOES HERE
\end{center}

The MI score ranges from 0 to +infinity, where 0 indicates no mutual information and higher values signify greater
similarity. To address certain limitations of the standard MI score, such as lack of normalization and agreement by chance, variations like the
Normalized Mutual Information (NMI) score and the Adjusted Mutual Information (AMI) score were developed.

NMI ranges from 0 to 1, while AMI ranges from -1 to 1. The key difference between AMI and NMI is that AMI adjusts for chance. 
AMI equals 0 when the clustering assignments are no more similar than would be expected by chance.

\textbf{When to use Mutual Info Scores?}

Use MI you need a basic measure of shared information between two clusterings. NMI when comparing clustering results with different numbers of
clusters, as it normalizes the score to a standard range. AMI when you want to account for chance.

\coloredboxes{
    \item Symmetric. Switching true label with predicted ones will return the same score.
    \item Lower and upper bounded in the case of the NMI and AMI variations.
    \item Can be used as a consensus score.
}
{
    \item Requires knowledge of ground truth classes.
    \item A permutation of the cluster label values doesn't change the score value in any way. eg. IM([0, 0, 1], [0, 0, 1]) = IM([0, 0, 1], [1, 0, 0])

}

% ---------- rand index ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Rand Index}
\subsection{Rand Index}

The Rand Index (RI) is a clustering metric that measures the similarity between two clusterings using the predicted labels generated by an algorithm
and the true labels or labels comming from a reference clustering.

\begin{center}
    % Formula of the type:
    % Number of agreeing pairs / Number of pairs
    FORMULA GOES HERE
\end{center}

The standard RI ranges from 0 to 1, where 1 indicates perfect agreement between the predicted and reference clusterings.
However, for random labelings, the RI does not yield values close to 0, as it lacks an adjustment for chance. To address this,
the Adjusted Rand Index (ARI) refines the RI by accounting for randomness. ARI values range from -0.5 to 1, where scores near 0
signify clustering results comparable to random labelings. RI is also equivalent to the accuracy score in a pairwise binary
classification task, evaluating the fraction of pairs correctly classified as "same cluster" (True Positives) or
"different cluster" (True Negatives).


\textbf{When to use Random Index Scores?}

Use RI/ARI when ground truth labels are available for benchmarking clustering performance. A comparison of consensus across
multiple clusterings is needed. Interpretability and connection to pairwise agreement are desired.

\coloredboxes{
    \item Symmetric. Switching true label with predicted ones will return the same score.
    \item Lower and upper bounded ranges for both RI and ARI.
    \item Can be used as a consensus score.
}
{
    \item Requires knowledge of ground truth classes.
    \item Similar to accuracy in binary classification, unadjusted RI is affected by class imbalance, which can result in high 
    RI scores even when the clusterings are significantly different.
}

% ---------- calinski harabasz index ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{CH Index}
\subsection{Calinski Harabasz Index}

The Calinski–Harabasz Index (CH Index), also known as the Variance Ratio Criterion, is a clustering evaluation metric that does
not require ground-truth labels. It measures the quality of clustering by comparing the dispersion between clusters to the
dispersion within clusters.

\begin{center}
    FORMULA GOES HERE
\end{center}

The CH Index is defined as the ratio of the between-clusters dispersion (BCSS) to the within-cluster dispersion (WCSS),
normalized by their respective degrees of freedom. We normalize BCSS and WCSS by their degrees of freedom to ensure comparability
across different values of $k$, avoiding artificial inflation of the score for higher cluster counts.

\textbf{When to use Calinski-Harabasz Index?}

Use CH Index when no ground-truth labels are available to validate the clustering quality. It can also be used to identify the
optimal number of clusters by maximizing the CH Index across different cluster counts.

\coloredboxes{
    \item The CH Index does not rely on labeled data.
    \item The use of degrees of freedom normalization ensures fair comparison across varying $k$ and sample sizes.
}
{
    \item The calculation assumes a Euclidean distance metric, which may limit its applicability for non-Euclidean data.
}

% ---------- contingency matrix ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Contingency Matrix}
\subsection{Contingency Matrix}

The Contingency Matrix is a tool for evaluating clustering performance by comparing the alignment of predicted cluster
assignments with ground-truth labels. It provides a detailed overview of how well the clusters correspond to the known
categories by counting overlaps between the two.

For a dataset of size $n$, let $k$ be the number of clusters and $C$ the number of ground-truth categories.
The Contingency Matrix is constructed such that the element at position  $(i,j)$ represents the number of data points that belong
to cluster $i$ and category $j$. If this sounds familiar to a Confusion Matrix, it is because it is. In fact, a Confusion Matrix
is a special case of a Contingency Matrix, where the matrix is square, and instead of clusters, we have classes.

\textbf{When to use a Contingency Matrix?}

The contingency matrix is particularly useful when ground-truth labels are available. When you want to analyze how well the
clustering results correspond to the known categories, especially for imbalanced datasets. Or when a deeper understanding
of misclassifications or overlaps between clusters and categories is needed. It most effective when combined with
complementary evaluation methods.

\coloredboxes{
    \item Provides a granular view of the correspondence between clusters and ground-truth categories,
    including misclassifications.
    \item Serves as a basis for other clustering evaluation metrics like Normalized Mutual Information (NMI) or
    Adjusted Rand Index (ARI).
}
{
    \item Requires ground-truth labels.
    \item For datasets with many clusters or categories, the matrix can become large and harder to interpret.
    \item Does not provide a single score.
}

% ---------- pair confusion matrix ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Pair Confusion Matrix}
\subsection{Pair Confusion Matrix}

The Pair Confusion Matrix is a clustering evaluation metric used to assess the similarity between two clusterings by
comparing pairwise relationships between data points. It is a $2x2$ matrix that extends the idea of the confusion matrix
to clustering scenarios, focusing on whether pairs of points are grouped together or not.

\begin{center}
    MATRIX GOES HERE
\end{center}

It has four key components, each representing a count of point pairs in specific configurations across two clusterings. It can
also be showen that $C_{00} + C_{11} + C_{10} + C_{01} = \frac{n(n-1)}{2}$

\textbf{When to use the Pair Confusion Matrix?}

Use the Pair Confusion Matrix when evaluating clustering algorithms that produce hard cluster assignments and when ground truth
is available.

\coloredboxes{
    \item Breaks down clustering agreements and disagreements into familiar components (TP, TN, FP, FN).
}
{
    \item Requires ground-truth labels.    
    \item Computation can be slow for large datasets due to the pairwise comparison of all points.
    \item Does not provide a single score.
}

\clearpage

\thispagestyle{customstyle}

Analyse the following example to understand how to build the matrix. Lets say we have the following clustering results.

\[
\begin{array}{l|cccccc}
    & \text{$obj_{1}$} & \text{$obj_{2}$} & \text{$obj_{3}$} & \text{$obj_{4}$} \\
\hline
\text{True Labels} & 0 & 0 & 1 & 0 \\
\text{Pred Labels} & 0 & 1 & 0 & 1 \\
\end{array}
\]

With this we can compute $C_{00}$, $C_{11}$, $C_{10}$ and $C_{01}$.

\begin{itemize}
    \item $C_{00}$: number of pairs with different true and pred labels = $(obj_{2}, obj_{3})$; $(obj_{3}, obj_{4})$; $(obj_{3}, obj_{2})$; $(obj_{4}, obj_{3})$ = $4$
    \item $C_{11}$: number of pairs with same true and pred labels = $(obj_{2}, obj_{4})$; $(obj_{4}, obj_{2})$ = $2$
    \item $C_{10}$: number of pairs with same true but different pred labels = $(obj_{1}, obj_{2})$; $(obj_{1}, obj_{4})$;
    $(obj_{2}, obj_{1})$; $(obj_{4}, obj_{1})$ = $4$
    \item $C_{01}$: number of pairs with different true but same pred labels = $(obj_{1}, obj_{3})$; $(obj_{3}, obj_{1})$ = $2$
\end{itemize}

Thus, the final Pair Confusion Matrix looks like:
\begin{center}
    \[
        C =
        \begin{bmatrix}
            4 & 2 \\
            4 & 2
        \end{bmatrix}
    \]
\end{center}

\orangebox{Did you know that...}
{MISSING FUN FACT}


\textbf{Other related metrics}

% ---------- Completeness Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Completeness Score}
\subsection{Completeness Score}

The Completeness Score evaluates the degree to which members of a ground-truth class are assigned to the same cluster in a
clustering solution. It is one of two complementary measures (the other being Homogeneity Score) used to assess the quality of
clustering. Completeness is mathematically defined using conditional entropy between cluster labels and assignments.

% \[
% \text{Completeness} = 1 - \frac{H(K \mid C)}{H(K)}
% \]

\begin{center}
    FORMULA GOES HERE
\end{center}

A perfect completeness score of 1 indicates that all members of a single class are contained within a single cluster. 
A score of 0 means the clustering completely fails to keep members of the same class together.

\textbf{When to use the Completeness Score?}

The Completeness Score is particularly useful when you need to evaluate how well a clustering algorithm preserves class membership
within individual clusters. It is commonly applied in scenarios such as document clustering, where documents belonging to the same
topic should ideally belong to one cluster.

\coloredboxes{
    \item Class-Focused Evaluation. Completeness measures how well class integrity is preserved in clustering,
    providing a clear diagnostic for one aspect of clustering quality.
    \item Lower and upper bounded.
    \item Clustering algorithm agnostic.
}
{
    \item Requires ground-truth labels.
    \item Trade-Off with Homogeneity. Completeness tends to decrease as completeness increases, creating a challenge when
    trying to optimize both simultaneously.
    \item Not normalized with regards to random labeling.
}

\clearpage

\thispagestyle{customstyle}

\textbf{Other related metrics}

The Completeness Score is best used in conjunction with the Homogeneity Score or V-Measure. While completeness focuses on
ensuring all members of a class are in the same cluster, homogeneity ensures that clusters do not mix members from different
classes. V-Measure provides a balanced harmonic mean of these two metrics, offering a single comprehensive evaluation of
clustering quality​.

% ---------- Homogeneity Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Homogeneity Score}
\subsection{Homogeneity Score}

The Homogeneity Score measures how well each cluster contains only data points from a single ground-truth class.
It is designed to assess the purity of clusters by evaluating the entropy of class distributions within each cluster.

\begin{center}
    FORMULA GOES HERE
\end{center}

A score of 1.0 indicates perfect homogeneity, meaning every cluster contains data points from only one class. 
A score of 0.0 indicates that the class distribution within clusters mirrors the overall dataset distribution,
showing no class distinction within clusters.

\textbf{When to use the Homogeneity Score?}

The Homogeneity Score is particularly valuable when evaluating the ability of a clustering algorithm to form well-defined,
class-pure clusters. Common applications include customer segmentation, where customers within a cluster should share similar characteristics.
Topic modeling, where documents within a cluster should belong to the same topic.

\coloredboxes{
    \item Cluster Purity Focus. Homogeneity emphasizes the purity of clusters, providing a clear diagnostic of whether
    clusters contain mixed classes.
    \item Lower and upper bounded.
    \item Clustering algorithm agnostic.
}
{
    \item Requires ground-truth labels.
    \item Trade-Off with Completeness. Homogeneity tends to decrease as completeness increases, creating a challenge when trying to optimize both simultaneously.
    \item Not normalized with regards to random labeling.
}


\clearpage

\thispagestyle{customstyle}

\textbf{Other related metrics}

Homogeneity is best used alongside Completeness or V-Measure. While Homogeneity ensures clusters are internally consistent with
respect to class membership, Completeness ensures that all members of a class are grouped together. V-Measure combines the two
into a single harmonic mean, providing a balanced evaluation of clustering quality

% ---------- V Measure ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{V Measure}
\subsection{V Measure}

V-Measure is an external clustering evaluation metric that combines the strengths of two complementary criteria: Homogeneity
(how pure clusters are with respect to ground-truth classes) and Completeness (how well all members of a class are grouped into
the same cluster). It is computed as the harmonic mean of these two scores, similar to how precision and recall are combined into
the F-score.

\begin{center}
    FORMULA GOES HERE
\end{center}

By design, V-Measure is bounded between 0.0 and 1.0, with higher scores indicating better clustering quality. A score of 1.0
means that both homogeneity and completeness are perfectly satisfied.

\textbf{When to use the V-measure?}

V-Measure is ideal for evaluating clustering solutions when you need a balanced assessment of both purity within clusters and the
completeness of class representation.

\coloredboxes{
    \item Combines homogeneity and completeness into a single score, providing a holistic evaluation of clustering quality.
    \item The $\beta$ parameter allows customization to prioritize either homogeneity or completeness based on specific use cases.
    \item Clustering algorithm agnostic.
}
{
    \item Requires ground-truth labels.
    \item V-Measure inherits the trade-off between homogeneity and completeness. Adjusting one often impacts the other, making it
    challenging to optimize both in some scenarios.
    \item Not normalized with regards to random labeling.
}

\clearpage

\thispagestyle{customstyle}

\textbf{Other related metrics}

V-Measure excels as a comprehensive metric for clustering evaluation. However, for specific insights, it can be supplemented with
individual Homogeneity and Completeness scores. Additionally, pair-based measures like Adjusted Rand Index (ARI) or
information-theoretic measures like Variation of Information (VI) may provide complementary perspectives in specific use cases​.

% ---------- Davis Bouldin Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Davis Bouldin Score}
\subsection{Davis Bouldin Score}

% ---------- Fowlkes Mallows Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Fowlkes Mallows Score}
\subsection{Fowlkes Mallows Score}

% ---------- Silhouette Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{Silhouette Score}
\subsection{Silhouette Score}

The Silhouette Coefficient measures how well a data point fits within its assigned cluster compared to other clusters. 
It quantifies how similar a sample is to its own cluster (cohesion) compared to the nearest neighboring cluster (separation).
The Silhouette Coefficient $s(i)$ for a given data point $i$ in the cluster $C_I$, where $|C_I|$ is the number of data points in $C_I$, is given by:

\begin{center}
\tikz{
\node[inner sep=2pt, font=\Large] (a) {
{
$\displaystyle
s(i) = 
\begin{cases}
\frac{{\color{cyan}b(i)} - {\color{nmlpurple}a(i)}}{max\left\{ {\color{nmlpurple}a(i)}, {\color{cyan}b(i)} \right\}}  & if\, |C_I| > 1 \\
0  & if\, |C_I| = 1
\end{cases}
$
}
};
\draw[-latex,cyan, semithick] ($(a.north)+(-0.9,0.05)$) to[bend right=15] node[pos=1, left] {measures cohesion} +(-1,.5); 
\draw[-latex,nmlpurple, semithick] ($(a.north)+(0.2,0.05)$) to[bend left=15] node[pos=1, right] {measures separation} +(1,.5); 
% \draw[-latex,nmlpurple, semithick] ($(b.north)+(0.1,0.05)$) to[bend left=15] node[pos=1, left] {True negatives} +(-1,-.5); 
}
\end{center}

Where  $a(i)$ is the mean distance between a sample and all other points in the same cluster (mean intra-cluster distance), 
and  $b(i)$ is the mean distance between a sample and all points in the nearest different cluster (mean nearest-cluster distance). 
The value of  $s(i)$ lies between -$1$ and  $1$, where  $s(i) \approx 1$ indicates that the data point is well-clustered, 
 $s(i) \approx 0$ means the data point is on or very close to the boundary between two clusters, and  $s(i) \approx -1$ 
suggests that the data point has been misclassified into the wrong cluster.

\textbf{When to use Silhouette Score?}

Use Silhouette Score when the ground truth labels are unknown and the evaluation must be performed using the model itself.

\coloredboxes{
\item It provides a clear and intuitive measure of how well clusters are separated, +1 highly dense and -1 for incorrect clustering.
}
{
\item Silhouette Score can generally be higher for convex clusters than other clusters, such as density based clusters.
\item Higher Silhouette Score doesn't necessarily indicate that the clusters will have equal sizes; the clusters can still vary in cardinality. 
}

% ----------  Consensus Score ----------
\clearpage
\thispagestyle{clusteringstyle}
\section{ Consensus Score}
\subsection{ Consensus Score}